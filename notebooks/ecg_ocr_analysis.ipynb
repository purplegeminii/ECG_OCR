{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Meter OCR â€” Dataset Analysis & Model Exploration\n",
    "\n",
    "This notebook provides interactive tools for:\n",
    "- Dataset quality inspection\n",
    "- Preprocessing visualisation\n",
    "- OCR accuracy exploration\n",
    "- Error analysis\n",
    "- Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import pytesseract\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "from scripts.utils import load_config, list_images, load_image_bgr, pair_images_with_gt, read_ground_truth\n",
    "\n",
    "cfg = load_config('../config/config.yaml')\n",
    "print('Config loaded OK')\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images and GT files in each directory\n",
    "dirs_to_check = [\n",
    "    '../raw_images',\n",
    "    '../preprocessed',\n",
    "    '../augmented',\n",
    "    '../training_data',\n",
    "    '../eval_data',\n",
    "]\n",
    "\n",
    "stats = []\n",
    "for d in dirs_to_check:\n",
    "    p = Path(d)\n",
    "    if p.exists():\n",
    "        images = list(p.glob('*.tif')) + list(p.glob('*.jpg')) + list(p.glob('*.png'))\n",
    "        gts = list(p.glob('*.gt.txt'))\n",
    "        stats.append({'directory': d, 'images': len(images), 'gt_files': len(gts)})\n",
    "    else:\n",
    "        stats.append({'directory': d, 'images': 0, 'gt_files': 0, 'note': 'not found'})\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "display(df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "x = np.arange(len(df))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, df['images'], w, label='Images', color='steelblue')\n",
    "ax.bar(x + w/2, df['gt_files'], w, label='GT files', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([Path(d).name for d in df['directory']], rotation=20)\n",
    "ax.set_title('Dataset Size by Directory')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "preprocess_image_in_memory = importlib.import_module('scripts.07_inference').preprocess_image_in_memory\n",
    "\n",
    "def show_preprocessing_steps(img_path_str):\n",
    "    img_path = Path(img_path_str)\n",
    "    img = load_image_bgr(img_path)\n",
    "    \n",
    "    pre_cfg = cfg.get('preprocessing', {})\n",
    "    block_size = pre_cfg.get('adaptive_thresh_block_size', 11)\n",
    "    thresh_c   = pre_cfg.get('adaptive_thresh_c', 2)\n",
    "    \n",
    "    # Steps\n",
    "    gray    = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray, h=10)\n",
    "    if block_size % 2 == 0: block_size += 1\n",
    "    binary  = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                     cv2.THRESH_BINARY, block_size, thresh_c)\n",
    "    kernel  = np.ones((1, 1), np.uint8)\n",
    "    cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    fig.suptitle(img_path.name, fontsize=12)\n",
    "    \n",
    "    imgs   = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB), gray, denoised, cleaned]\n",
    "    titles = ['Original', 'Grayscale', 'Denoised', 'Binary']\n",
    "    cmaps  = [None, 'gray', 'gray', 'gray']\n",
    "    \n",
    "    for ax, im, title, cmap in zip(axes, imgs, titles, cmaps):\n",
    "        ax.imshow(im, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage: change path to one of your images\n",
    "# show_preprocessing_steps('../raw_images/sample_meter.jpg')\n",
    "\n",
    "raw_imgs = list(Path('../raw_images').glob('*.jpg')) + list(Path('../raw_images').glob('*.png'))\n",
    "if raw_imgs:\n",
    "    @interact(image=[str(p) for p in raw_imgs[:20]])\n",
    "    def interactive_preprocess(image):\n",
    "        show_preprocessing_steps(image)\n",
    "else:\n",
    "    print('Add images to raw_images/ to see preprocessing visualisation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ground Truth Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "gt_dir = Path('../ground_truth')\n",
    "gt_files = list(gt_dir.glob('*.gt.txt')) if gt_dir.exists() else []\n",
    "\n",
    "if gt_files:\n",
    "    gt_data = []\n",
    "    for f in gt_files:\n",
    "        text = f.read_text().strip()\n",
    "        gt_data.append({\n",
    "            'file': f.name,\n",
    "            'text': text,\n",
    "            'length': len(text),\n",
    "            'num_digits': sum(c.isdigit() for c in text),\n",
    "            'digit_ratio': sum(c.isdigit() for c in text) / max(len(text), 1),\n",
    "            'has_reading': bool(re.search(r'\\b\\d{4,6}\\b', text)),\n",
    "        })\n",
    "    \n",
    "    import re\n",
    "    df_gt = pd.DataFrame(gt_data)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].hist(df_gt['length'], bins=20, color='steelblue', edgecolor='white')\n",
    "    axes[0].set_title('GT Text Length Distribution')\n",
    "    axes[0].set_xlabel('Characters')\n",
    "    \n",
    "    axes[1].hist(df_gt['digit_ratio'], bins=20, color='coral', edgecolor='white')\n",
    "    axes[1].set_title('Digit Ratio Distribution')\n",
    "    axes[1].set_xlabel('Fraction of digits')\n",
    "    \n",
    "    has_reading = df_gt['has_reading'].value_counts()\n",
    "    axes[2].pie(has_reading.values, labels=['Has reading', 'No reading'],\n",
    "                autopct='%1.0f%%', colors=['green', 'orange'])\n",
    "    axes[2].set_title('Samples with Meter Readings')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Total GT files: {len(gt_files)}')\n",
    "    print(df_gt.describe()[['length', 'digit_ratio']])\n",
    "else:\n",
    "    print('No ground truth files found. Run annotation first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OCR Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation report if available\n",
    "report_path = Path('../results/evaluation_report.csv')\n",
    "\n",
    "if report_path.exists():\n",
    "    df_eval = pd.read_csv(report_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('OCR Evaluation Results', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # CER distribution\n",
    "    axes[0, 0].hist(df_eval['cer'], bins=25, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "    axes[0, 0].axvline(df_eval['cer'].mean(), color='red', linestyle='--',\n",
    "                        label=f\"Mean: {df_eval['cer'].mean():.3f}\")\n",
    "    axes[0, 0].axvline(0.02, color='green', linestyle='--', label='Target: 0.02')\n",
    "    axes[0, 0].set_title('CER Distribution')\n",
    "    axes[0, 0].set_xlabel('Character Error Rate')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # WER distribution\n",
    "    axes[0, 1].hist(df_eval['wer'], bins=25, color='coral', edgecolor='white', alpha=0.8)\n",
    "    axes[0, 1].axvline(df_eval['wer'].mean(), color='red', linestyle='--',\n",
    "                        label=f\"Mean: {df_eval['wer'].mean():.3f}\")\n",
    "    axes[0, 1].set_title('WER Distribution')\n",
    "    axes[0, 1].set_xlabel('Word Error Rate')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Confidence distribution\n",
    "    axes[1, 0].hist(df_eval['confidence'], bins=25, color='purple', edgecolor='white', alpha=0.8)\n",
    "    axes[1, 0].axvline(60, color='orange', linestyle='--', label='Confidence threshold: 60')\n",
    "    axes[1, 0].set_title('OCR Confidence Distribution')\n",
    "    axes[1, 0].set_xlabel('Confidence Score')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # CER vs Confidence scatter\n",
    "    axes[1, 1].scatter(df_eval['confidence'], df_eval['cer'], alpha=0.4, s=20)\n",
    "    axes[1, 1].set_title('CER vs Confidence')\n",
    "    axes[1, 1].set_xlabel('Confidence')\n",
    "    axes[1, 1].set_ylabel('CER')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary stats\n",
    "    print('\\n=== Evaluation Summary ===')\n",
    "    print(f\"Samples:     {len(df_eval)}\")\n",
    "    print(f\"Mean CER:    {df_eval['cer'].mean():.4f} ({df_eval['cer'].mean()*100:.2f}%)\")\n",
    "    print(f\"Median CER:  {df_eval['cer'].median():.4f}\")\n",
    "    print(f\"Mean WER:    {df_eval['wer'].mean():.4f}\")\n",
    "    print(f\"Flagged:     {df_eval.get('flagged_for_review', pd.Series([False]*len(df_eval))).sum()}\")\n",
    "    \n",
    "    # Worst 10\n",
    "    print('\\n=== Worst 10 Predictions ===')\n",
    "    worst = df_eval.nlargest(10, 'cer')[['image', 'ground_truth', 'predicted', 'cer', 'confidence']]\n",
    "    display(worst)\n",
    "else:\n",
    "    print('No evaluation report found. Run: python scripts/06_evaluate.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Augmentation Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dir = Path('../augmented')\n",
    "aug_images = list(augmented_dir.glob('*_aug*.tif')) if augmented_dir.exists() else []\n",
    "\n",
    "if aug_images:\n",
    "    # Show original + 5 augmented variants for a sample\n",
    "    sample_stem = aug_images[0].stem.rsplit('_aug', 1)[0]\n",
    "    \n",
    "    orig_candidates = (\n",
    "        list(Path('../preprocessed').glob(f'{sample_stem}.tif')) +\n",
    "        list(Path('../preprocessed').glob(f'{sample_stem}.jpg'))\n",
    "    )\n",
    "    \n",
    "    variants = sorted(augmented_dir.glob(f'{sample_stem}_aug*.tif'))[:5]\n",
    "    \n",
    "    all_imgs = (orig_candidates[:1] + variants)[:6]\n",
    "    labels   = ['Original'] + [f'Aug {i}' for i in range(1, len(all_imgs))]\n",
    "    \n",
    "    if all_imgs:\n",
    "        fig, axes = plt.subplots(1, len(all_imgs), figsize=(4 * len(all_imgs), 4))\n",
    "        if len(all_imgs) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, img_path, label in zip(axes, all_imgs, labels):\n",
    "            img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(label, fontsize=9)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Augmentation variants: {sample_stem}', fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f'Total augmented images: {len(aug_images)}')\n",
    "else:\n",
    "    print('No augmented images found. Run: python scripts/03_augment.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
